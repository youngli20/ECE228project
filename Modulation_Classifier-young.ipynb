{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulationClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(ModulationClassifier, self).__init__()\n",
    "        \n",
    "        # Treating input as 2x128 image with 1 channel (depth 1)\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(1,3), padding=(0,2)), torch.nn.BatchNorm2d(256))\n",
    "        self.conv2 = torch.nn.Sequential(torch.nn.Conv2d(in_channels=256, out_channels=80, kernel_size=(2,3), padding=(0,2)), torch.nn.BatchNorm2d(80))\n",
    "        #self.fc1   = torch.nn.Linear(in_features=10560, out_features=256)\n",
    "        #self.fc2   = torch.nn.Linear(in_features=256, out_features=num_classes)\n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Linear(in_features=10560, out_features=256),\n",
    "                                              torch.nn.ReLU(True),\n",
    "                                             torch.nn.Linear(in_features=256, out_features=11))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.conv1(x))\n",
    "        y2 = F.relu(self.conv2(y1))\n",
    "        y3 = torch.flatten(y2, 1)\n",
    "        y4 = self.classifier(y3)\n",
    "        return y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
      "[-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "(0,) (0,) (0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimage = np.zeros((220000,2,128), dtype='float32')\\nmodulation = np.zeros((220000), dtype='int32')\\nsnr = np.zeros((220000), dtype='int32')\\ncnt = 0\\n\\nfor m_cnt,m in enumerate(modulation_types,0):\\n    for s in snr_types:\\n        print(cnt, m_cnt,s)\\n        #print(input_data_dict[(m,s)].shape)\\n        image[cnt:cnt+1000,:,:] = np.array(input_data_dict[(m,s)])\\n        modulation[cnt:cnt+1000] = np.array([m_cnt for _ in range(0,1000)])\\n        snr[cnt:cnt+1000] = np.array([s for _ in range(0,1000)])\\n        #print(image[cnt:cnt+1000,:,:], modulation[cnt:cnt+1000], snr[cnt:cnt+1000])\\n        cnt += 1000\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'RML2016.10a_dict.pkl'\n",
    "f = open(fname,'rb')\n",
    "input_data_dict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "\"\"\"\n",
    "Data size is 1000*2*128\n",
    "Consider it as 1000 images of size 1*2*128. Where 1 is the color channel.\n",
    "We need the data in this format:\n",
    "\n",
    "image - mod - snr\n",
    "image - mod - snr\n",
    "image - mod - snr\n",
    "\n",
    "Create 3 arrays:\n",
    "image, mod, snr\n",
    "\"\"\"\n",
    "\n",
    "input_data_dict_keys = sorted(input_data_dict.keys())\n",
    "\n",
    "modulation_types    = [input_data_dict_keys[i*20][0] for i in range(0,11)]\n",
    "snr_types           = [input_data_dict_keys[i][1] for i in range(0,20)]\n",
    "\n",
    "print(modulation_types)\n",
    "print(snr_types)\n",
    "\n",
    "\n",
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[10:]:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)\n",
    "\n",
    "\"\"\"\n",
    "image = np.zeros((220000,2,128), dtype='float32')\n",
    "modulation = np.zeros((220000), dtype='int32')\n",
    "snr = np.zeros((220000), dtype='int32')\n",
    "cnt = 0\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types:\n",
    "        print(cnt, m_cnt,s)\n",
    "        #print(input_data_dict[(m,s)].shape)\n",
    "        image[cnt:cnt+1000,:,:] = np.array(input_data_dict[(m,s)])\n",
    "        modulation[cnt:cnt+1000] = np.array([m_cnt for _ in range(0,1000)])\n",
    "        snr[cnt:cnt+1000] = np.array([s for _ in range(0,1000)])\n",
    "        #print(image[cnt:cnt+1000,:,:], modulation[cnt:cnt+1000], snr[cnt:cnt+1000])\n",
    "        cnt += 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative SNR type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 2, 128) (110000,) (110000,)\n"
     ]
    }
   ],
   "source": [
    "image1 = []\n",
    "modulation1 = []\n",
    "snr1 = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[:10]:\n",
    "        #print(m,str(s))\n",
    "        image1.extend(input_data_dict[(m,s)])\n",
    "        modulation1.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr1.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image1 = np.array(image1)\n",
    "modulation1 = np.array(modulation1)\n",
    "snr1 = np.array(snr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X,Y,Z, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rX, rY, rZ = torch.unsqueeze(torch.from_numpy(self.X[idx]),dim=0).float(), torch.tensor(self.Y[idx]), torch.tensor(self.Z[idx])\n",
    "        if(self.transform):\n",
    "            rX, rY, rZ = self.transform(self.X[idx]),torch.tensor(self.Y[idx]),torch.tensor(self.Z[idx])\n",
    "            \n",
    "        return rX, rY, rZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = [image, modulation, snr]\n",
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "#rX, rY, rZ = torch.unsqueeze(torch.from_numpy(image[0]),dim=0).float(), torch.tensor(modulation[0]), torch.tensor(snr[0])\n",
    "#print(type(rY))\n",
    "transformed_dataset2 = myDataset(image1, modulation1, snr1,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "split = 0.5\n",
    "train_indices, test_indices = l[:int(split*dataset_len)],l[int(split*dataset_len):]\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_sampler=torch.utils.data.SequentialSampler(train_indices)\n",
    "# test_sampler=torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n",
    "testloader2 = torch.utils.data.DataLoader(transformed_dataset2, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112779238889384\n",
      "0.5048488896946575\n",
      "0.4445687431235646\n",
      "0.39716524918412055\n",
      "0.373049094233402\n",
      "0.3410013764403587\n",
      "0.30595230162143705\n",
      "0.28070604104635327\n",
      "0.2584812211089356\n",
      "0.2301949530146843\n",
      "0.20797962376544643\n",
      "0.1862474104866039\n",
      "0.16476252461935198\n",
      "0.15026853127881537\n",
      "0.1371432600672855\n",
      "0.12440655713164529\n",
      "0.11337660551937513\n",
      "0.10481442762322205\n",
      "0.09568323979890624\n",
      "0.08852812072565389\n",
      "0.08504276727001335\n",
      "0.08154836076116839\n",
      "0.08059946495259918\n",
      "0.07965323317709357\n",
      "0.07809596571118332\n",
      "0.07890419444545757\n",
      "0.0813767320887987\n",
      "0.08188883928192216\n",
      "0.07455072303324245\n",
      "0.07446715266898621\n",
      "0.070875500601738\n",
      "0.06943702712481799\n",
      "0.06917436970181243\n",
      "0.06672620515144148\n",
      "0.06675414936833604\n",
      "0.0657540614435146\n",
      "0.06531778218354596\n",
      "0.06546319530800332\n",
      "0.06383538608287656\n",
      "0.06333799651975548\n",
      "0.062443177687913874\n",
      "0.06326699684005838\n",
      "0.0623754377281943\n",
      "0.0608642595105393\n",
      "0.05971358559679153\n",
      "0.057609103316831035\n",
      "0.05749940107208352\n",
      "0.0572495705513067\n",
      "0.057438565444114596\n",
      "0.05712867930618137\n",
      "0.05500647279238978\n",
      "0.05415913262945968\n",
      "0.052878265308086266\n",
      "0.05258215596076361\n",
      "0.05311164032061433\n",
      "0.052823958398644316\n",
      "0.05113058524249598\n",
      "0.050839095389427144\n",
      "0.052227055645266245\n",
      "0.055340752542711966\n",
      "0.05126294724989769\n",
      "0.04930172289700009\n",
      "0.04899026790070672\n",
      "0.049337752535939215\n",
      "0.04832423235787902\n",
      "0.04823572328963945\n",
      "0.04789522614243419\n",
      "0.04637730845878291\n",
      "0.04589128382503986\n",
      "0.04481670671944008\n",
      "0.043646941463961156\n",
      "0.04327926087777975\n",
      "0.042580654680035836\n",
      "0.047588971681719605\n",
      "0.0599666828939388\n",
      "0.08467159327552762\n",
      "0.0847138564600501\n",
      "0.06545623141492522\n",
      "0.058718757340034775\n",
      "0.05120436124676882\n",
      "0.04578996803178344\n",
      "0.04359581701804039\n",
      "0.04100300334393978\n",
      "0.03939228722349156\n",
      "0.03927656299066405\n",
      "0.03995031373098839\n",
      "0.03686923050256662\n",
      "0.03540031323425991\n",
      "0.034845013970639126\n",
      "0.03625068163802458\n",
      "0.03543880430245122\n",
      "0.03446419640162657\n",
      "0.03430920028409293\n",
      "0.033050829388721045\n",
      "0.034872474781302516\n",
      "0.03345420026155405\n",
      "0.031041906470822733\n",
      "0.032517307572240056\n",
      "0.030454051208704015\n",
      "0.029435335229649102\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        #x_mean = torch.mean(x, 1, keepdims=True)\n",
    "        #print(x.size(), y.size(), z.size(), x_mean.size())\n",
    "        #x = x - x_mean\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        #print(torch.argmax(y_pred, 1).size())\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #print( y, torch.argmax(y_pred,1))\n",
    "        #break\n",
    "        \n",
    "    print(running_loss/len(trainloader))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 36.846061 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_1.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "for i,data in enumerate(testloader2):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR -20 Accuracy 0.091091\n",
      "SNR -18 Accuracy 0.091273\n",
      "SNR -16 Accuracy 0.090818\n",
      "SNR -14 Accuracy 0.091364\n",
      "SNR -12 Accuracy 0.092364\n",
      "SNR -10 Accuracy 0.094545\n",
      "SNR -8 Accuracy 0.105727\n",
      "SNR -6 Accuracy 0.147818\n",
      "SNR -4 Accuracy 0.239818\n",
      "SNR -2 Accuracy 0.501909\n",
      "SNR 0 Accuracy 0.721704\n",
      "SNR 2 Accuracy 0.780959\n",
      "SNR 4 Accuracy 0.792556\n",
      "SNR 6 Accuracy 0.802469\n",
      "SNR 8 Accuracy 0.806615\n",
      "SNR 10 Accuracy 0.811562\n",
      "SNR 12 Accuracy 0.819533\n",
      "SNR 14 Accuracy 0.805682\n",
      "SNR 16 Accuracy 0.809837\n",
      "SNR 18 Accuracy 0.808909\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train data with -SNR values to see accuracy.  \n",
    "Only for 50 epochs because it is taking too long and also because I ran it for 100 epochs and it crashed the kernel. \n",
    "Also the loss seemed to converge so I think 50 is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = [image, modulation, snr]\n",
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "#rX, rY, rZ = torch.unsqueeze(torch.from_numpy(image[0]),dim=0).float(), torch.tensor(modulation[0]), torch.tensor(snr[0])\n",
    "#print(type(rY))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "split = 0.75\n",
    "train_indices, test_indices = l[:int(split*dataset_len)],l[int(split*dataset_len):]\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4354270675385645\n",
      "1.2671230855838274\n",
      "1.205181092763132\n",
      "1.1589781196542488\n",
      "1.1108236409897028\n",
      "1.055221280802128\n",
      "0.9880070736703946\n",
      "0.9070671696533529\n",
      "0.8088782590027004\n",
      "0.6947427920823873\n",
      "0.5712802789238997\n",
      "0.4525671663672425\n",
      "0.3679952258402987\n",
      "0.263254493301691\n",
      "0.19409776880990628\n",
      "0.17017362544587417\n",
      "0.14242465124111767\n",
      "0.10132375520098116\n",
      "0.0848478510889203\n",
      "0.07058570082913074\n",
      "0.06296918105131896\n",
      "0.09884231392034265\n",
      "0.07313690985184769\n",
      "0.05605750500058481\n",
      "0.04661101148456566\n",
      "0.07513432854194511\n",
      "0.05971565712965274\n",
      "0.04867663065967865\n",
      "0.04471762504053208\n",
      "0.041795845970857975\n",
      "0.04547784792533679\n",
      "0.05162625261401945\n",
      "0.04301408659377994\n",
      "0.08034036000396393\n",
      "0.04908708581166674\n",
      "0.05770407168035822\n",
      "0.042265140799473425\n",
      "0.1250227796245915\n",
      "0.0754421825152497\n",
      "0.05140304068724314\n",
      "0.04028850000265033\n",
      "0.03603624861774057\n",
      "0.033957370070292967\n",
      "0.03309194813518561\n",
      "0.03378450180035691\n",
      "0.0313549674700859\n",
      "0.03093013798659162\n",
      "0.09955358650506467\n",
      "0.08511214223076669\n",
      "0.044782661060376684\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        #x_mean = torch.mean(x, 1, keepdims=True)\n",
    "        #print(x.size(), y.size(), z.size(), x_mean.size())\n",
    "        #x = x - x_mean\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        #print(torch.argmax(y_pred, 1).size())\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #print( y, torch.argmax(y_pred,1))\n",
    "        #break\n",
    "        \n",
    "    print(running_loss/len(trainloader))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 70.363030 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_1.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    #print(i)\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "for i,data in enumerate(testloader2):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR -20 Accuracy 0.643856\n",
      "SNR -18 Accuracy 0.631244\n",
      "SNR -16 Accuracy 0.645190\n",
      "SNR -14 Accuracy 0.645403\n",
      "SNR -12 Accuracy 0.662121\n",
      "SNR -10 Accuracy 0.676494\n",
      "SNR -8 Accuracy 0.708203\n",
      "SNR -6 Accuracy 0.735676\n",
      "SNR -4 Accuracy 0.783731\n",
      "SNR -2 Accuracy 0.831499\n",
      "SNR 0 Accuracy 0.666178\n",
      "SNR 2 Accuracy 0.713018\n",
      "SNR 4 Accuracy 0.727437\n",
      "SNR 6 Accuracy 0.750997\n",
      "SNR 8 Accuracy 0.758201\n",
      "SNR 10 Accuracy 0.764021\n",
      "SNR 12 Accuracy 0.743248\n",
      "SNR 14 Accuracy 0.759420\n",
      "SNR 16 Accuracy 0.756642\n",
      "SNR 18 Accuracy 0.754411\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we have discussed that having -SNR values in our model will hinder our accuracy and we have shown that that is true.\n",
    "\n",
    "\n",
    "### Now, lets try to setup a validation set and do some hyperparameter tuning.\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[10:]:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of trainLoader:  602\n",
      "Size of testLoader:  16500\n",
      "Size of ValLoader:  129\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "transformed_dataset2 = myDataset(image1, modulation1, snr1,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "#train_split = 0.7\n",
    "#val_split = 0.05\n",
    "#test_split = 0.25\n",
    "\n",
    "split = 0.7\n",
    "\n",
    "#train_indices, test_indices, val_indices = l[:int(train_split*dataset_len)], l[int(test_split*dataset_len):], l[int(val_split*dataset_len):]\n",
    "train_indices, test_indices = l[:int(split*dataset_len)], l[int(split*dataset_len):]\n",
    "\n",
    "valsplit = 0.5 ##meaning, 70% of +SNR data used for training, 15% goes to validation 15% goes to testing\n",
    "test_indices, val_indices = test_indices[:int(valsplit*len(test_indices))], test_indices[int(valsplit*len(test_indices)):]\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "val_sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n",
    "testloader2 = torch.utils.data.DataLoader(transformed_dataset2, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=val_sampler, pin_memory=True)\n",
    "\n",
    "\n",
    "print(\"Size of trainLoader: \", len(trainloader))\n",
    "print(\"Size of testLoader: \", len(testloader))\n",
    "print(\"Size of ValLoader: \", len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0 Loss:  0.3219956862817571\n",
      "Validation Loss:  0.4148435557997504\n",
      "Epoch # 1 Loss:  0.2991983654392122\n",
      "Validation Loss:  0.41464636076328365\n",
      "Epoch # 2 Loss:  0.27297129922629987\n",
      "Validation Loss:  0.44669210333232734\n",
      "Epoch # 3 Loss:  0.24366953882367112\n",
      "Validation Loss:  0.43245743873507475\n",
      "Epoch # 4 Loss:  0.2215228399812581\n",
      "Validation Loss:  0.4425177835216818\n",
      "Epoch # 5 Loss:  0.1996993170153857\n",
      "Validation Loss:  0.473610941753831\n",
      "Epoch # 6 Loss:  0.1785317664699895\n",
      "Validation Loss:  0.48840392352074613\n",
      "Epoch # 7 Loss:  0.15941674502387\n",
      "Validation Loss:  0.5186032828434494\n",
      "Epoch # 8 Loss:  0.14515100936854006\n",
      "Validation Loss:  0.5614757496257161\n",
      "Epoch # 9 Loss:  0.13318917302544728\n",
      "Validation Loss:  0.5579406919405442\n",
      "Epoch # 10 Loss:  0.11725822088627325\n",
      "Validation Loss:  0.5900259244349576\n",
      "Epoch # 11 Loss:  0.10937063996850256\n",
      "Validation Loss:  0.6372968429742858\n",
      "Epoch # 12 Loss:  0.10253903066795134\n",
      "Validation Loss:  0.6728853775087253\n",
      "Epoch # 13 Loss:  0.09779362951055713\n",
      "Validation Loss:  0.6648055670797363\n",
      "Epoch # 14 Loss:  0.08894441565158359\n",
      "Validation Loss:  0.6908509176830913\n",
      "Epoch # 15 Loss:  0.08571419389021159\n",
      "Validation Loss:  0.7054221999737643\n",
      "Epoch # 16 Loss:  0.08003726318800371\n",
      "Validation Loss:  0.7353959718877955\n",
      "Epoch # 17 Loss:  0.07704471356770723\n",
      "Validation Loss:  0.7437282837638559\n",
      "Epoch # 18 Loss:  0.07409284602343244\n",
      "Validation Loss:  0.7715616600458012\n",
      "Epoch # 19 Loss:  0.07605710828002307\n",
      "Validation Loss:  0.8010765937871711\n",
      "Epoch # 20 Loss:  0.07888813266078895\n",
      "Validation Loss:  0.7935983226742855\n",
      "Epoch # 21 Loss:  0.07880782319907334\n",
      "Validation Loss:  0.802522210418716\n",
      "Epoch # 22 Loss:  0.07620727609954403\n",
      "Validation Loss:  0.802601348984149\n",
      "Epoch # 23 Loss:  0.07195692542210172\n",
      "Validation Loss:  0.8079655190077863\n",
      "Epoch # 24 Loss:  0.0698068996031221\n",
      "Validation Loss:  0.8295602904733761\n",
      "Epoch # 25 Loss:  0.06958572463166476\n",
      "Validation Loss:  0.8367371434389159\n",
      "Epoch # 26 Loss:  0.06497142822218693\n",
      "Validation Loss:  0.8404724452384683\n",
      "Epoch # 27 Loss:  0.0643222300502846\n",
      "Validation Loss:  0.8572982958582944\n",
      "Epoch # 28 Loss:  0.06432396265681202\n",
      "Validation Loss:  0.8602673792561819\n",
      "Epoch # 29 Loss:  0.06188993360166534\n",
      "Validation Loss:  0.890040512694869\n",
      "Epoch # 30 Loss:  0.061954038335239374\n",
      "Validation Loss:  0.8960764879404113\n",
      "Epoch # 31 Loss:  0.06010673606638298\n",
      "Validation Loss:  0.8876004112783329\n",
      "Epoch # 32 Loss:  0.06097638509251747\n",
      "Validation Loss:  0.8988201334032901\n",
      "Epoch # 33 Loss:  0.05904364965212504\n",
      "Validation Loss:  0.9063322800536489\n",
      "Epoch # 34 Loss:  0.06023464366744127\n",
      "Validation Loss:  0.8912721796091213\n",
      "Epoch # 35 Loss:  0.057998654961685006\n",
      "Validation Loss:  0.9339064463626506\n",
      "Epoch # 36 Loss:  0.057555582683743826\n",
      "Validation Loss:  0.9293778541014176\n",
      "Epoch # 37 Loss:  0.05726489571984424\n",
      "Validation Loss:  0.9250277765961581\n",
      "Epoch # 38 Loss:  0.055933689853281676\n",
      "Validation Loss:  0.969618885092033\n",
      "Epoch # 39 Loss:  0.05781025058927132\n",
      "Validation Loss:  0.9345545454542766\n",
      "Epoch # 40 Loss:  0.05397739128961516\n",
      "Validation Loss:  0.9546179311682087\n",
      "Epoch # 41 Loss:  0.05498992606527187\n",
      "Validation Loss:  0.9617723443711451\n",
      "Epoch # 42 Loss:  0.05383328283731625\n",
      "Validation Loss:  0.9693449620128602\n",
      "Epoch # 43 Loss:  0.052743847113709115\n",
      "Validation Loss:  0.9756452367287274\n",
      "Epoch # 44 Loss:  0.0535527572803125\n",
      "Validation Loss:  0.9867016090903171\n",
      "Epoch # 45 Loss:  0.05401763977154941\n",
      "Validation Loss:  1.0186616496522298\n",
      "Epoch # 46 Loss:  0.0537393132094727\n",
      "Validation Loss:  1.0248164241166078\n",
      "Epoch # 47 Loss:  0.05470467020952424\n",
      "Validation Loss:  0.9936823673950609\n",
      "Epoch # 48 Loss:  0.0537944489910555\n",
      "Validation Loss:  1.0297548720078875\n",
      "Epoch # 49 Loss:  0.05452603413615116\n",
      "Validation Loss:  1.0180609443391015\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "historyEpoch = []\n",
    "historyLoss = []\n",
    "historyValLoss = []\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    historyEpoch.append(epoch)\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    total = 0\n",
    "    val_loss = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            x, y, z = data\n",
    "            x, y, z = x.to(device), y.to(device), z.to(device)\n",
    "            \n",
    "            y_pred = net.forward(x)\n",
    "            batch_loss = criterion(y_pred, y)\n",
    "            val_loss += batch_loss.item()\n",
    "            \n",
    "            total += 1\n",
    "            \n",
    "    print(\"Epoch #\", epoch, \"Loss: \", running_loss/len(trainloader))\n",
    "    print(\"Validation Loss: \", val_loss/total)\n",
    "    historyLoss.append(running_loss/len(trainloader))\n",
    "    historyValLoss.append(val_loss/total)\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 25.135178 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_1.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "for i,data in enumerate(testloader2):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR -20 Accuracy 0.091273\n",
      "SNR -18 Accuracy 0.091091\n",
      "SNR -16 Accuracy 0.090273\n",
      "SNR -14 Accuracy 0.091636\n",
      "SNR -12 Accuracy 0.093000\n",
      "SNR -10 Accuracy 0.095000\n",
      "SNR -8 Accuracy 0.112545\n",
      "SNR -6 Accuracy 0.161182\n",
      "SNR -4 Accuracy 0.296455\n",
      "SNR -2 Accuracy 0.566182\n",
      "SNR 0 Accuracy 0.727879\n",
      "SNR 2 Accuracy 0.787247\n",
      "SNR 4 Accuracy 0.803755\n",
      "SNR 6 Accuracy 0.811676\n",
      "SNR 8 Accuracy 0.794765\n",
      "SNR 10 Accuracy 0.821236\n",
      "SNR 12 Accuracy 0.823600\n",
      "SNR 14 Accuracy 0.825201\n",
      "SNR 16 Accuracy 0.811506\n",
      "SNR 18 Accuracy 0.806549\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## i forgot to divide historyValLoss by total (which was 129)\n",
    "tempList = np.divide(historyValLoss, 129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7f4718b64f98>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Training performance')\n",
    "plt.plot(historyEpoch, historyLoss, label='train loss')\n",
    "plt.plot(historyEpoch, tempList, label='val_error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
