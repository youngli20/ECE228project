{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModulationClassifier(torch.nn.Module):\n",
    "    def __init__(self, num_classes=11):\n",
    "        super(ModulationClassifier, self).__init__()\n",
    "        \n",
    "        # Treating input as 2x128 image with 1 channel (depth 1)\n",
    "        self.conv1 = torch.nn.Sequential(torch.nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(1,3), padding=(0,2)), torch.nn.BatchNorm2d(256))\n",
    "        self.conv2 = torch.nn.Sequential(torch.nn.Conv2d(in_channels=256, out_channels=80, kernel_size=(2,3), padding=(0,2)), torch.nn.BatchNorm2d(80))\n",
    "        #self.fc1   = torch.nn.Linear(in_features=10560, out_features=256)\n",
    "        #self.fc2   = torch.nn.Linear(in_features=256, out_features=num_classes)\n",
    "        self.classifier = torch.nn.Sequential(torch.nn.Linear(in_features=10560, out_features=256),\n",
    "                                              torch.nn.ReLU(True),\n",
    "                                             torch.nn.Linear(in_features=256, out_features=11))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y1 = F.relu(self.conv1(x))\n",
    "        y2 = F.relu(self.conv2(y1))\n",
    "        y3 = torch.flatten(y2, 1)\n",
    "        y4 = self.classifier(y3)\n",
    "        return y4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['8PSK', 'AM-DSB', 'AM-SSB', 'BPSK', 'CPFSK', 'GFSK', 'PAM4', 'QAM16', 'QAM64', 'QPSK', 'WBFM']\n",
      "[-20, -18, -16, -14, -12, -10, -8, -6, -4, -2, 0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
      "(0,) (0,) (0,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nimage = np.zeros((220000,2,128), dtype='float32')\\nmodulation = np.zeros((220000), dtype='int32')\\nsnr = np.zeros((220000), dtype='int32')\\ncnt = 0\\n\\nfor m_cnt,m in enumerate(modulation_types,0):\\n    for s in snr_types:\\n        print(cnt, m_cnt,s)\\n        #print(input_data_dict[(m,s)].shape)\\n        image[cnt:cnt+1000,:,:] = np.array(input_data_dict[(m,s)])\\n        modulation[cnt:cnt+1000] = np.array([m_cnt for _ in range(0,1000)])\\n        snr[cnt:cnt+1000] = np.array([s for _ in range(0,1000)])\\n        #print(image[cnt:cnt+1000,:,:], modulation[cnt:cnt+1000], snr[cnt:cnt+1000])\\n        cnt += 1000\\n\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = 'RML2016.10a_dict.pkl'\n",
    "f = open(fname,'rb')\n",
    "input_data_dict = pickle.load(f, encoding='latin1')\n",
    "\n",
    "\"\"\"\n",
    "Data size is 1000*2*128\n",
    "Consider it as 1000 images of size 1*2*128. Where 1 is the color channel.\n",
    "We need the data in this format:\n",
    "\n",
    "image - mod - snr\n",
    "image - mod - snr\n",
    "image - mod - snr\n",
    "\n",
    "Create 3 arrays:\n",
    "image, mod, snr\n",
    "\"\"\"\n",
    "\n",
    "input_data_dict_keys = sorted(input_data_dict.keys())\n",
    "\n",
    "modulation_types    = [input_data_dict_keys[i*20][0] for i in range(0,11)]\n",
    "snr_types           = [input_data_dict_keys[i][1] for i in range(0,20)]\n",
    "\n",
    "print(modulation_types)\n",
    "print(snr_types)\n",
    "\n",
    "\n",
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[10:]:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)\n",
    "\n",
    "\"\"\"\n",
    "image = np.zeros((220000,2,128), dtype='float32')\n",
    "modulation = np.zeros((220000), dtype='int32')\n",
    "snr = np.zeros((220000), dtype='int32')\n",
    "cnt = 0\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types:\n",
    "        print(cnt, m_cnt,s)\n",
    "        #print(input_data_dict[(m,s)].shape)\n",
    "        image[cnt:cnt+1000,:,:] = np.array(input_data_dict[(m,s)])\n",
    "        modulation[cnt:cnt+1000] = np.array([m_cnt for _ in range(0,1000)])\n",
    "        snr[cnt:cnt+1000] = np.array([s for _ in range(0,1000)])\n",
    "        #print(image[cnt:cnt+1000,:,:], modulation[cnt:cnt+1000], snr[cnt:cnt+1000])\n",
    "        cnt += 1000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add negative SNR type data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110000, 2, 128) (110000,) (110000,)\n"
     ]
    }
   ],
   "source": [
    "image1 = []\n",
    "modulation1 = []\n",
    "snr1 = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[:10]:\n",
    "        #print(m,str(s))\n",
    "        image1.extend(input_data_dict[(m,s)])\n",
    "        modulation1.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr1.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image1 = np.array(image1)\n",
    "modulation1 = np.array(modulation1)\n",
    "snr1 = np.array(snr1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X,Y,Z, transform=None):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.Z = Z\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        rX, rY, rZ = torch.unsqueeze(torch.from_numpy(self.X[idx]),dim=0).float(), torch.tensor(self.Y[idx]), torch.tensor(self.Z[idx])\n",
    "        if(self.transform):\n",
    "            rX, rY, rZ = self.transform(self.X[idx]),torch.tensor(self.Y[idx]),torch.tensor(self.Z[idx])\n",
    "            \n",
    "        return rX, rY, rZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = [image, modulation, snr]\n",
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "#rX, rY, rZ = torch.unsqueeze(torch.from_numpy(image[0]),dim=0).float(), torch.tensor(modulation[0]), torch.tensor(snr[0])\n",
    "#print(type(rY))\n",
    "transformed_dataset2 = myDataset(image1, modulation1, snr1,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "split = 0.5\n",
    "train_indices, test_indices = l[:int(split*dataset_len)],l[int(split*dataset_len):]\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "# train_sampler=torch.utils.data.SequentialSampler(train_indices)\n",
    "# test_sampler=torch.utils.data.SequentialSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n",
    "testloader2 = torch.utils.data.DataLoader(transformed_dataset2, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8112779238889384\n",
      "0.5048488896946575\n",
      "0.4445687431235646\n",
      "0.39716524918412055\n",
      "0.373049094233402\n",
      "0.3410013764403587\n",
      "0.30595230162143705\n",
      "0.28070604104635327\n",
      "0.2584812211089356\n",
      "0.2301949530146843\n",
      "0.20797962376544643\n",
      "0.1862474104866039\n",
      "0.16476252461935198\n",
      "0.15026853127881537\n",
      "0.1371432600672855\n",
      "0.12440655713164529\n",
      "0.11337660551937513\n",
      "0.10481442762322205\n",
      "0.09568323979890624\n",
      "0.08852812072565389\n",
      "0.08504276727001335\n",
      "0.08154836076116839\n",
      "0.08059946495259918\n",
      "0.07965323317709357\n",
      "0.07809596571118332\n",
      "0.07890419444545757\n",
      "0.0813767320887987\n",
      "0.08188883928192216\n",
      "0.07455072303324245\n",
      "0.07446715266898621\n",
      "0.070875500601738\n",
      "0.06943702712481799\n",
      "0.06917436970181243\n",
      "0.06672620515144148\n",
      "0.06675414936833604\n",
      "0.0657540614435146\n",
      "0.06531778218354596\n",
      "0.06546319530800332\n",
      "0.06383538608287656\n",
      "0.06333799651975548\n",
      "0.062443177687913874\n",
      "0.06326699684005838\n",
      "0.0623754377281943\n",
      "0.0608642595105393\n",
      "0.05971358559679153\n",
      "0.057609103316831035\n",
      "0.05749940107208352\n",
      "0.0572495705513067\n",
      "0.057438565444114596\n",
      "0.05712867930618137\n",
      "0.05500647279238978\n",
      "0.05415913262945968\n",
      "0.052878265308086266\n",
      "0.05258215596076361\n",
      "0.05311164032061433\n",
      "0.052823958398644316\n",
      "0.05113058524249598\n",
      "0.050839095389427144\n",
      "0.052227055645266245\n",
      "0.055340752542711966\n",
      "0.05126294724989769\n",
      "0.04930172289700009\n",
      "0.04899026790070672\n",
      "0.049337752535939215\n",
      "0.04832423235787902\n",
      "0.04823572328963945\n",
      "0.04789522614243419\n",
      "0.04637730845878291\n",
      "0.04589128382503986\n",
      "0.04481670671944008\n",
      "0.043646941463961156\n",
      "0.04327926087777975\n",
      "0.042580654680035836\n",
      "0.047588971681719605\n",
      "0.0599666828939388\n",
      "0.08467159327552762\n",
      "0.0847138564600501\n",
      "0.06545623141492522\n",
      "0.058718757340034775\n",
      "0.05120436124676882\n",
      "0.04578996803178344\n",
      "0.04359581701804039\n",
      "0.04100300334393978\n",
      "0.03939228722349156\n",
      "0.03927656299066405\n",
      "0.03995031373098839\n",
      "0.03686923050256662\n",
      "0.03540031323425991\n",
      "0.034845013970639126\n",
      "0.03625068163802458\n",
      "0.03543880430245122\n",
      "0.03446419640162657\n",
      "0.03430920028409293\n",
      "0.033050829388721045\n",
      "0.034872474781302516\n",
      "0.03345420026155405\n",
      "0.031041906470822733\n",
      "0.032517307572240056\n",
      "0.030454051208704015\n",
      "0.029435335229649102\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        #x_mean = torch.mean(x, 1, keepdims=True)\n",
    "        #print(x.size(), y.size(), z.size(), x_mean.size())\n",
    "        #x = x - x_mean\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        #print(torch.argmax(y_pred, 1).size())\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #print( y, torch.argmax(y_pred,1))\n",
    "        #break\n",
    "        \n",
    "    print(running_loss/len(trainloader))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 36.846061 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_1.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "for i,data in enumerate(testloader2):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR -20 Accuracy 0.091091\n",
      "SNR -18 Accuracy 0.091273\n",
      "SNR -16 Accuracy 0.090818\n",
      "SNR -14 Accuracy 0.091364\n",
      "SNR -12 Accuracy 0.092364\n",
      "SNR -10 Accuracy 0.094545\n",
      "SNR -8 Accuracy 0.105727\n",
      "SNR -6 Accuracy 0.147818\n",
      "SNR -4 Accuracy 0.239818\n",
      "SNR -2 Accuracy 0.501909\n",
      "SNR 0 Accuracy 0.721704\n",
      "SNR 2 Accuracy 0.780959\n",
      "SNR 4 Accuracy 0.792556\n",
      "SNR 6 Accuracy 0.802469\n",
      "SNR 8 Accuracy 0.806615\n",
      "SNR 10 Accuracy 0.811562\n",
      "SNR 12 Accuracy 0.819533\n",
      "SNR 14 Accuracy 0.805682\n",
      "SNR 16 Accuracy 0.809837\n",
      "SNR 18 Accuracy 0.808909\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train data with -SNR values to see accuracy.  \n",
    "Only for 50 epochs because it is taking too long and also because I ran it for 100 epochs and it crashed the kernel. \n",
    "Also the loss seemed to converge so I think 50 is okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset = [image, modulation, snr]\n",
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "#rX, rY, rZ = torch.unsqueeze(torch.from_numpy(image[0]),dim=0).float(), torch.tensor(modulation[0]), torch.tensor(snr[0])\n",
    "#print(type(rY))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "split = 0.75\n",
    "train_indices, test_indices = l[:int(split*dataset_len)],l[int(split*dataset_len):]\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1534653807333273\n",
      "1.1108761246352232\n",
      "1.0486861754757488\n",
      "0.9816833774248759\n",
      "0.8983660638794418\n",
      "0.7996799930121548\n",
      "0.6895921946727028\n",
      "0.5682824958433477\n",
      "0.45163143693014635\n",
      "0.34802322284888854\n",
      "0.25600399027845655\n",
      "0.21272994066740192\n",
      "0.1542755943022726\n",
      "0.1229332166009171\n",
      "0.10040969415748305\n",
      "0.08198989807635315\n",
      "0.09131508919504262\n",
      "0.11216388211636118\n",
      "0.109900020086026\n",
      "0.08718934008606183\n",
      "0.03792133937741435\n",
      "0.035993806691479314\n",
      "0.034323670258817746\n",
      "0.035837647760676786\n",
      "0.03369242050215718\n",
      "0.03304080106606779\n",
      "0.03238219274857709\n",
      "0.033089952196839245\n",
      "0.032620657186291015\n",
      "0.07077711184704026\n",
      "0.04435664832938549\n",
      "0.036097655777436816\n",
      "0.05442213343037653\n",
      "0.06511778993488744\n",
      "0.039111996147521706\n",
      "0.03281014509094778\n",
      "0.03075558760658253\n",
      "0.02985337482000044\n",
      "0.029203764326119607\n",
      "0.029196680845447288\n",
      "0.028644101102222767\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        #x_mean = torch.mean(x, 1, keepdims=True)\n",
    "        #print(x.size(), y.size(), z.size(), x_mean.size())\n",
    "        #x = x - x_mean\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        #print(torch.argmax(y_pred, 1).size())\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        #print( y, torch.argmax(y_pred,1))\n",
    "        #break\n",
    "        \n",
    "    print(running_loss/len(trainloader))\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 49.214545 %\n"
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_2.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    #print(i)\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "# for i,data in enumerate(testloader2):\n",
    "#     x,y,z = data\n",
    "#     x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "#     y_pred = model1.forward(x)\n",
    "#     correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "#     snr_value = z.item()\n",
    "\n",
    "#     snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "#     snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "#     total += 1 #Increase by batch size\n",
    "\n",
    "print('Accuracy of the network on the test images: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR -20 Accuracy 0.099211\n",
      "SNR -18 Accuracy 0.093333\n",
      "SNR -16 Accuracy 0.098722\n",
      "SNR -14 Accuracy 0.110259\n",
      "SNR -12 Accuracy 0.137463\n",
      "SNR -10 Accuracy 0.198134\n",
      "SNR -8 Accuracy 0.265328\n",
      "SNR -6 Accuracy 0.348733\n",
      "SNR -4 Accuracy 0.476824\n",
      "SNR -2 Accuracy 0.581071\n",
      "SNR 0 Accuracy 0.667037\n",
      "SNR 2 Accuracy 0.688477\n",
      "SNR 4 Accuracy 0.748724\n",
      "SNR 6 Accuracy 0.744385\n",
      "SNR 8 Accuracy 0.761368\n",
      "SNR 10 Accuracy 0.766753\n",
      "SNR 12 Accuracy 0.763316\n",
      "SNR 14 Accuracy 0.755922\n",
      "SNR 16 Accuracy 0.762380\n",
      "SNR 18 Accuracy 0.764727\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Above we have discussed that having -SNR values in our model will hinder our accuracy and we have shown that that is true.\n",
    "\n",
    "\n",
    "### Now, lets try to setup a validation set and do some hyperparameter tuning.\n",
    "\n",
    "https://towardsdatascience.com/how-to-train-an-image-classifier-in-pytorch-and-use-it-to-perform-basic-inference-on-single-images-99465a1e9bf5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0,) (0,) (0,)\n"
     ]
    }
   ],
   "source": [
    "image = []\n",
    "modulation = []\n",
    "snr = []\n",
    "\n",
    "print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "\n",
    "for m_cnt,m in enumerate(modulation_types,0):\n",
    "    for s in snr_types[10:]:\n",
    "        #print(m,str(s))\n",
    "        image.extend(input_data_dict[(m,s)])\n",
    "        modulation.extend([m_cnt for _ in range(0,1000)])\n",
    "        snr.extend([s for _ in range(0,1000)])\n",
    "        \n",
    "#print(np.shape(image), np.shape(modulation), np.shape(snr))\n",
    "image = np.array(image)\n",
    "modulation = np.array(modulation)\n",
    "snr = np.array(snr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77000 16500 16500\n",
      "Size of trainLoader:  602\n",
      "Size of testLoader:  16500\n",
      "Size of ValLoader:  129\n"
     ]
    }
   ],
   "source": [
    "transformed_dataset = myDataset(image, modulation, snr,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "#transformed_dataset2 = myDataset(image1, modulation1, snr1,transforms.Compose([transforms.ToTensor(),transforms.Normalize((0,),(1,))]))\n",
    "\n",
    "dataset_len = len(image)\n",
    "l = list(range(0, dataset_len))\n",
    "np.random.shuffle(l)\n",
    "#train_split = 0.7\n",
    "#val_split = 0.05\n",
    "#test_split = 0.25\n",
    "\n",
    "split = 0.7\n",
    "\n",
    "#train_indices, test_indices, val_indices = l[:int(train_split*dataset_len)], l[int(test_split*dataset_len):], l[int(val_split*dataset_len):]\n",
    "train_indices, test_indices = l[:int(split*dataset_len)], l[int(split*dataset_len):]\n",
    "\n",
    "valsplit = 0.5 ##meaning, 70% of +SNR data used for training, 15% goes to validation 15% goes to testing\n",
    "test_indices, val_indices = test_indices[:int(valsplit*len(test_indices))], test_indices[int(valsplit*len(test_indices)):]\n",
    "\n",
    "print(len(train_indices), len(test_indices),len(val_indices))\n",
    "\n",
    "train_sampler=torch.utils.data.SubsetRandomSampler(train_indices)\n",
    "test_sampler=torch.utils.data.SubsetRandomSampler(test_indices)\n",
    "val_sampler=torch.utils.data.SubsetRandomSampler(val_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=train_sampler, pin_memory=True)\n",
    "testloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=1, shuffle=False, num_workers=2, sampler=test_sampler, pin_memory=True)\n",
    "#testloader2 = torch.utils.data.DataLoader(transformed_dataset2, batch_size=1, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "valloader = torch.utils.data.DataLoader(transformed_dataset, batch_size=128, shuffle=False, num_workers=2, sampler=val_sampler, pin_memory=True)\n",
    "\n",
    "\n",
    "print(\"Size of trainLoader: \", len(trainloader))\n",
    "print(\"Size of testLoader: \", len(testloader))\n",
    "print(\"Size of ValLoader: \", len(valloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ModulationClassifier(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(1, 256, kernel_size=(1, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(256, 80, kernel_size=(2, 3), stride=(1, 1), padding=(0, 2))\n",
      "    (1): BatchNorm2d(80, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=10560, out_features=256, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Linear(in_features=256, out_features=11, bias=True)\n",
      "  )\n",
      ")\n",
      "conv1.0.weight                          \ttorch.Size([256, 1, 1, 3])    \t       768\n",
      "conv1.0.bias                            \ttorch.Size([256])             \t       256\n",
      "conv1.1.weight                          \ttorch.Size([256])             \t       256\n",
      "conv1.1.bias                            \ttorch.Size([256])             \t       256\n",
      "conv2.0.weight                          \ttorch.Size([80, 256, 2, 3])   \t    122880\n",
      "conv2.0.bias                            \ttorch.Size([80])              \t        80\n",
      "conv2.1.weight                          \ttorch.Size([80])              \t        80\n",
      "conv2.1.bias                            \ttorch.Size([80])              \t        80\n",
      "classifier.0.weight                     \ttorch.Size([256, 10560])      \t   2703360\n",
      "classifier.0.bias                       \ttorch.Size([256])             \t       256\n",
      "classifier.2.weight                     \ttorch.Size([11, 256])         \t      2816\n",
      "classifier.2.bias                       \ttorch.Size([11])              \t        11\n"
     ]
    }
   ],
   "source": [
    "net = ModulationClassifier()\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Assuming that we are on a CUDA machine, this should print a CUDA device:\n",
    "print(device)\n",
    "net.to(device)\n",
    "\n",
    "print(net)\n",
    "for name, param in net.named_parameters():\n",
    "    #if param.requires_grad:\n",
    "    print('{:s}\\t{:s}\\t{:s}'.format(name.ljust(40), str(param.size()).ljust(30),str(param.nelement()).rjust(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_function(net, criterion):\n",
    "    PATH = './temp_model_val.pth'\n",
    "    torch.save(net.state_dict(), PATH) \n",
    "\n",
    "    model1 = ModulationClassifier()\n",
    "    model1.to(device)\n",
    "    model1.load_state_dict(torch.load(PATH))\n",
    "    \n",
    "    val_loss = 0\n",
    "    total = 0\n",
    "    model1.eval()\n",
    "    for i, data in enumerate(valloader):\n",
    "        xv, yv, zv = data\n",
    "        xv, yv, zv = xv.to(device), yv.to(device), zv.to(device)\n",
    "\n",
    "        yv_pred = model1.forward(xv)\n",
    "        batchv_loss = criterion(yv_pred, yv.long())\n",
    "        val_loss += batchv_loss.item()\n",
    "\n",
    "        total += 128\n",
    "        \n",
    "    #print(val_loss)\n",
    "        \n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0 Correct: 52237 Loss:  0.7209601718127926 Val Loss 0.5248683778814567\n",
      "Epoch # 1 Correct: 58922 Loss:  0.47437593306219855 Val Loss 0.4251047934210578\n",
      "Epoch # 2 Correct: 60554 Loss:  0.4210029142936599 Val Loss 0.4088867373706758\n",
      "Epoch # 3 Correct: 62089 Loss:  0.38223698195626965 Val Loss 0.39935529486153476\n",
      "Epoch # 4 Correct: 63176 Loss:  0.3537909023686501 Val Loss 0.4109544585379519\n",
      "Epoch # 5 Correct: 64088 Loss:  0.32829212295850646 Val Loss 0.39014030311458797\n",
      "Epoch # 6 Correct: 65333 Loss:  0.29996658710645285 Val Loss 0.3968972099381824\n",
      "Epoch # 7 Correct: 66511 Loss:  0.27216769767263005 Val Loss 0.412706860969233\n",
      "Epoch # 8 Correct: 67564 Loss:  0.24631090308740686 Val Loss 0.43173877998839977\n",
      "Epoch # 9 Correct: 68460 Loss:  0.2255395496158901 Val Loss 0.4560415223818417\n",
      "Epoch # 10 Correct: 69515 Loss:  0.20061323952635263 Val Loss 0.45992083651150845\n",
      "Epoch # 11 Correct: 70200 Loss:  0.18056590992499427 Val Loss 0.47970535496408623\n",
      "Epoch # 12 Correct: 70668 Loss:  0.16966591039022735 Val Loss 0.5041017171948455\n",
      "Epoch # 13 Correct: 71422 Loss:  0.14860995038353725 Val Loss 0.5685272842876671\n",
      "Epoch # 14 Correct: 71681 Loss:  0.14035060687417603 Val Loss 0.5506852142108504\n",
      "Epoch # 15 Correct: 72340 Loss:  0.12284863705551902 Val Loss 0.5918357279873634\n",
      "Epoch # 16 Correct: 72798 Loss:  0.10870255701912201 Val Loss 0.6247828602790833\n",
      "Epoch # 17 Correct: 73014 Loss:  0.10095428024415558 Val Loss 0.6686204332251882\n",
      "Epoch # 18 Correct: 73072 Loss:  0.1009395404709534 Val Loss 0.6726362044497054\n",
      "Epoch # 19 Correct: 73285 Loss:  0.09412725714106496 Val Loss 0.7001083026560702\n",
      "Epoch # 20 Correct: 73386 Loss:  0.08970358236337025 Val Loss 0.7319774482139322\n",
      "Epoch # 21 Correct: 73525 Loss:  0.08453059577045845 Val Loss 0.7366164423698602\n",
      "Epoch # 22 Correct: 73699 Loss:  0.08121501201411022 Val Loss 0.7459725670574248\n",
      "Epoch # 23 Correct: 73673 Loss:  0.07958524766381199 Val Loss 0.7720500889212586\n",
      "Epoch # 24 Correct: 73755 Loss:  0.07626435937873549 Val Loss 0.793563326661901\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 25\n",
    "epoch_loss_array = np.zeros(num_epochs)\n",
    "\n",
    "historyEpoch = []\n",
    "historyLoss = []\n",
    "historyValLoss = []\n",
    "\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0\n",
    "    correct_test = 0\n",
    "    historyEpoch.append(epoch)\n",
    "    \n",
    "    for i, data in enumerate(trainloader):\n",
    "        x,y,z = data\n",
    "        x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = net.forward(x)   # Run batch\n",
    "        correct_test += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "        loss = criterion(y_pred, y.long())  # Wants indexes for labels, *not* one-hot encodings.\n",
    "        #print(y_pred)\n",
    "        #print(loss.item())\n",
    "        loss.backward()                               # Compute backprop\n",
    "        optimizer.step()                              # Move a step in the right direction\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    valloss = validation_function(net, criterion)\n",
    "    #correct, total, snr_accuracy = testing_function(net, criterion)\n",
    "    #plt.figure(figsize=(15,7))\n",
    "    #plt.plot(list(range(0,20,2)),[values[0]/values[1] for key,values in snr_accuracy.items()])\n",
    "    \n",
    "    \n",
    "#     total = 0\n",
    "#     val_loss = 0\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for j, dataval in enumerate(valloader):\n",
    "#             xv, yv, zv = dataval\n",
    "#             xv, yv, zv = xv.to(device), yv.to(device), zv.to(device)\n",
    "            \n",
    "#             yv_pred = net.forward(xv)\n",
    "#             batchv_loss = criterion(yv_pred, yv)\n",
    "#             val_loss += batchv_loss.item()\n",
    "            \n",
    "#             total += 128\n",
    "            \n",
    "    print(\"Epoch #\", epoch, 'Correct:', correct_test,\"Loss: \", running_loss/len(trainloader), 'Val Loss', valloss/len(valloader))\n",
    "    historyLoss.append(running_loss/len(trainloader))\n",
    "    historyValLoss.append(valloss/len(valloader))\n",
    "\n",
    "print('Finished Training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-38e6147a7954>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0msnr_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0msnr_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0msnr_accuracy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msnr_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "PATH = './temp_model_xyz.pth'\n",
    "torch.save(net.state_dict(), PATH) \n",
    "\n",
    "model1 = ModulationClassifier()\n",
    "model1.to(device)\n",
    "model1.load_state_dict(torch.load(PATH))\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "snr_accuracy = dict()\n",
    "for i in range(-20,20,2):\n",
    "    k = str(i)\n",
    "    snr_accuracy[k] = [0,0]\n",
    "    \n",
    "model1.eval()\n",
    "for i,data in enumerate(testloader):\n",
    "    x,y,z = data\n",
    "    x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "    y_pred = model1.forward(x)\n",
    "    correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_value = z.item()\n",
    "\n",
    "    snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "    snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "    total += 1 #Increase by batch size\n",
    "\n",
    "# for i,data in enumerate(testloader2):\n",
    "#     x,y,z = data\n",
    "#     x,y,z = x.to(device), y.to(device), z.to(device)\n",
    "\n",
    "#     y_pred = model1.forward(x)\n",
    "#     correct += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "#     snr_value = z.item()\n",
    "\n",
    "#     snr_accuracy[str(int(snr_value))][0] += torch.sum((torch.argmax(y_pred,dim=1) == y)).item()\n",
    "#     snr_accuracy[str(int(snr_value))][1] += 1\n",
    "    \n",
    "#     total += 1 #Increase by batch size\n",
    "\n",
    "print('%d %d Accuracy of the network on the test images: %f %%' % (correct, total,\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNR 0 Accuracy 0.743125\n",
      "SNR 2 Accuracy 0.796493\n",
      "SNR 4 Accuracy 0.808184\n",
      "SNR 6 Accuracy 0.825416\n",
      "SNR 8 Accuracy 0.824169\n",
      "SNR 10 Accuracy 0.821322\n",
      "SNR 12 Accuracy 0.828877\n",
      "SNR 14 Accuracy 0.832139\n",
      "SNR 16 Accuracy 0.823091\n",
      "SNR 18 Accuracy 0.817518\n"
     ]
    }
   ],
   "source": [
    "for key, values in snr_accuracy.items():\n",
    "    if(values[1]):\n",
    "        print('SNR {:s} Accuracy {:f}'.format(key, values[0]/values[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "## i forgot to divide historyValLoss by total (which was 129)\n",
    "#tempList = np.divide(historyValLoss, 129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fbfa17a86a0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "plt.title('Training performance')\n",
    "plt.plot(historyEpoch, historyLoss, label='train loss')\n",
    "plt.plot(historyEpoch, historyValLoss, label='val_error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
